---
title: K8S-指南
date: 2022-4-15 13:26
permalink: /pages/devops002/
titleTag: 原创
categories:
  - DevOps
tags:
  - DevOps
  - 流水线
author:
  name: hq
  link: https://gitee.com/huang-qing
---

# k8s-常用命令

## 使用yaml文件创建Deployment

```sql
kubectl create -f docs/user-guide/nginx-deployment.yaml --record
# 创建成功后会输出：deployment "nginx-deployment" created
```
<!-- more -->
## 创建

```bash
# 通过命令行创建
kubectl create namespace xxx    # 创建命名空间
kubectl create -f ./pod.json  --namespace=xxx    # 通过json文件创建pod
# 通过文件创建，my-namespace.yaml
apiVersion: V1
kind: Namespace
metadata:
     name: new-namespace

kubectl create -f my-namespace.yaml
```

## 删除命名空间

```cpp
kubectl delete namespace xxx
# default和kube-system不可删
```

## 查看命名空间

```csharp
kubectl get namespace
```

## 在指定命名空间创建各种对象

```sql
kubectl create -f https://k8s.io/docs/tasks/administer-cluster/quota-pod.yaml --namespace=xxx  #  创建ResourceQuota对象
kubectl get resourcequota pod-demo --namespace=xxx --output=yaml     # 查看ResourceQuota对象的详细信息
kubectl create -f https://k8s.io/docs/tasks/administer-cluster/quota-pod-deployment.yaml --namespace=xxx    # 尝试在命名空间创建Deployment      
kubectl get deployment pod-quota-demo --namespace=xxx --output=yaml   # 查看Deployment的详细信息
kubectl get all --namespace    # 列出所有不同的资源对象
```

## 列出所有运行的pod信息

```csharp
kubectl get pods --namespace=xxx     # --namespace指定命名空间
kubectl get pods -o wide     # 列出pod以及运行pod节点信息
kubectl get -o json pod web-pod-xxx      # 以json格式输出一个pod信息
kubectl get -f pod.yaml -o json             # 以pod.yaml文件中指定的资源对象和名称输出json格式的Pod信息
```

## 列出所有replication controllers和service信息

```csharp
kubectl get rc,services
```

## 列出所有不同的资源对象

```sql
kubectl  get  all
```

## 创建一个名称为yyy的deployment，运行xxx镜像

```lua
kubectl  create deployment  yyy  --image=xxx
```

## 创建并运行一个或多个容器镜像

```dockerfile
kubectl run nginx  --image=nginx      # 启动nginx实例
kubectl  run hazelcast  --image=hazecast  --port=5701   # 启动hazelcast实例，暴露容器端口5701
kubectl run hazelcast --image=hazelcast --env="DNS_DOMAIN=cluster" --env="POD_NAMESPACE=default"   # 在容器中设置环境变量“DNS_DOMAIN = cluster”和“POD_NAMESPACE = default”
kubectl run nginx --image=nginx --replicas=5     # 启动nginx实例，设置副本数5
```

## 查看节点labels信息

```sql
kubectl get note --show-labels
```

## 查看节点详细信息

```scss
kubectl describe node master(节点名)
```

## 修复pod问题

```cmake
kubectl get deploy shutao -n fst-test  -o yaml > shutao.yaml    # 将pod生成yaml文件，修改故障
kubectl  apply -f shutao.yaml  # 使用这个命令更新pod
kubectl  replace -f  shutao.yaml     # 也可以使用这个命令更新
kubectl  patch pod xxxxx -p '{"metadata":{"labels":{"app":"nginx-3"}}}'    # 在容器运行时，直接对容器进行修改，此命令修改了label参数
```

## 两种方式查看pod的详细信息

```sql
kubectl get po xxxxx -o yaml -n nnnnn

kubectl describe po xxxxxxx -n nnnnn
```

## 使用scale修改副本数

```python
kubectl scale rc rc-nginx-3    --replicas=4   # 使用scale修改副本数为4, 此命令仅限于非deployment下的pod
kubectl autoscale rc rc-nginx-3 --min=1 --max=4    # 使用autoscale根据负载自动在制定范围内进行扩容或缩容，此命令为设置副本范围在1～4个之间
```

## 使用selector决定了请求会被发送给集群里的哪些 pod

![img](https://img2020.cnblogs.com/blog/1633991/202104/1633991-20210409172549937-1118566214.png)
这里的定义是所有包含「app: k8s-demo」这个标签的 pod。可以使用以下命令查看创建的pod所有的标签
`kubectl describe pods | grep Labels`



## 其他命令

```shell
## K8S常用命令：

## 查看pod: kubectl get pod -n名称空间

## 进入容器： kubectl exec -it pod名字 -n名称空间 bash

## 查看类命令：

## 获取节点和服务版本信息，并查看附加信息

kubectl get nodes -o wide || kubectl get nodes -o wide -A

## 获取pod信息，默认是default名称空间

kubectl get pod

## 获取pod信息，默认是default名称空间，并查看附加信息【如：pod的IP及在哪个节点运行】

kubectl get pod -o wide || kubectl get pod -o wide -A

## 获取指定名称空间的pod

kubectl get pod -n kube-system

## 获取指定名称空间中的指定pod

kubectl get pod -n kube-system podName

## 获取所有名称空间的pod

kubectl get pod -A

## 查看pod的详细信息，以yaml格式或json格式显示

kubectl get pods -o yaml

kubectl get pods -o json

## 查看pod的标签信息

kubectl get pod -A --show-labels

## 根据Selector（label query）来查询pod

kubectl get pod -A --selector=“k8s-app=kube-dns”

## 查看运行pod的环境变量

kubectl exec podName env

## 查看指定pod的日志

kubectl logs -f --tail 500 -n kube-system kube-apiserver-k8s-master

## 查看所有名称空间的service信息

kubectl get svc -A

## 查看指定名称空间的service信息

kubectl get svc -n kube-system

## 查看componentstatuses信息

kubectl get cs

## 查看所有configmaps信息

kubectl get cm -A

## 查看所有serviceaccounts信息

kubectl get sa -A

## 查看所有daemonsets信息

kubectl get ds -A

## 查看所有deployments信息

kubectl get deploy -A

## 查看所有replicasets信息

kubectl get rs -A

## 查看所有statefulsets信息

kubectl get sts -A

## 查看所有jobs信息

kubectl get jobs -A

## 查看所有ingresses信息

kubectl get ing -A

## 查看有哪些名称空间

kubectl get ns

## 查看pod的描述信息

kubectl describe pod podName

kubectl describe pod -n kube-system kube-apiserver-k8s-master

## 查看指定名称空间中指定deploy的描述信息

kubectl describe deploy -n kube-system coredns

## 查看node或pod的资源使用情况

## 需要heapster 或metrics-server支持

kubectl top node

kubectl top pod

## 查看集群信息

kubectl cluster-info 或 kubectl cluster-info dump

## 查看各组件信息【172.16.1.110为master机器】

kubectl -s https://172.16.1.110:6443 get componentstatuses

## 进阶命令操作：

## kubectl exec：进入pod启动的容器

kubectl exec -it podName -n nsName /bin/sh ## 进入容器

kubectl exec -it podName -n nsName /bin/bash ## 进入容器

## kubectl label：添加label值

kubectl label nodes k8s-node01 zone=north ## 为指定节点添加标签

kubectl label nodes k8s-node01 zone- ## 为指定节点删除标签

kubectl label pod podName -n nsName role-name=test ## 为指定pod添加标签

kubectl label pod podName -n nsName role-name=dev --overwrite ## 修改lable标签值

kubectl label pod podName -n nsName role-name- ## 删除lable标签

## kubectl滚动升级； 通过 kubectl apply -f myapp-deployment-v1.yaml 启动deploy

kubectl apply -f myapp-deployment-v2.yaml ## 通过配置文件滚动升级

kubectl set image deploy/myapp-deployment myapp=“registry.cn-beijing.aliyuncs.com/google_registry/myapp:v3” ## 通过命令滚动升级

kubectl rollout undo deploy/myapp-deployment 或者 kubectl rollout undo deploy myapp-deployment ## pod回滚到前一个版本

kubectl rollout undo deploy/myapp-deployment --to-revision=2 ## 回滚到指定历史版本

## kubectl scale：动态伸缩

kubectl scale deploy myapp-deployment --replicas=5 ##  动态伸缩

kubectl scale --replicas=8 -f myapp-deployment-v2.yaml ## 动态伸缩【根据资源类型和

## 操作类命令：

## 创建资源

kubectl create -f xxx.yaml

## 应用资源

kubectl apply -f xxx.yaml

## 应用资源，该目录下的所有 .yaml, .yml, 或 .json 文件都会被使用

kubectl apply -f

## 创建test名称空间

kubectl create namespace test

## 删除资源

kubectl delete -f xxx.yaml

kubectl delete -f

## 删除指定的pod

kubectl delete pod podName

## 删除指定名称空间的指定pod

kubectl delete pod -n test podName

## 删除其他资源

kubectl delete svc svcName

kubectl delete deploy deployName

kubectl delete ns nsName

## 强制删除

kubectl delete pod podName -n nsName --grace-period=0 --force

kubectl delete pod podName -n nsName --grace-period=1

kubectl delete pod podName -n nsName --now

## 编辑资源

kubectl edit pod podName
```



# docker & rancher & k8s安装部署

## 参考文件

[docker & rancher & k8s安装部署](.\docker & rancher & k8s 安装部署)

## centos7配置准备

```shell
# 所有机器都要操作

# 关闭防火墙 或者开放端口
sudo systemctl stop firewalld
sudo systemctl disable firewalld

#----------------------------------------
#设置hostname && 设置hosts

## 这里我的ip为 192.168.238.129
## 主节点
hostnamectl set-hostname k8s-master-01
## 从节点
hostnamectl set-hostname k8s-node-01

vi /etc/hosts
##主节点
192.168.238.129 k8s-master-01
##从节点
192.168.238.130  k8s-node-01

#----------------------------------------

##设置ip为静态

vi /ect/sysconfig/network-scripts/ifcfg-ens33

BOOTPROTO=static
IPADDR=192.168.238.129 #所属主机ip
GATEWAY=192.168.238.2  #网关地址  可在 vm 中的虚拟网络编辑器查看
NETWORK=255.255.255.0
DNS1=8.8.8.8
DNS2=192.168.238.2
ONBOOT=yes

systemctl restart network

# 安装ifconfig 如果主机有下面命令 则无需操作
yum search ifconfig
yum install -y net-tools.x86_64
yum -y install wget
yum install -y lrzsz


#----------------------------------------

# 安装时间同步插件
https://blog.csdn.net/vah101/article/details/91868147

```

## docker 安装

```shell
# 依赖
yum -y install yum-utils
# 安装docker 源
yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
# 查看可安装docker版本
yum list docker-ce --showduplicates|sort -r
# 指定版本安装
yum install -y "docker-ce-18*" 
# 开机自启并启动
systemctl enable docker && systemctl restart docker 

# 镜像加速
vi /etc/docker/daemon.json 
{
  "registry-mirrors": ["http://f1361db2.m.daocloud.io","https://nidf33r8.mirror.aliyuncs.com","https://docker.mirrors.ustc.edu.cn","https://hub-mirror.c.163.com","https://registry.docker-cn.com"],
  "exec-opts": ["native.cgroupdriver=systemd"]
}

```



## k8s-安装部署

centos7安装k8s指定版本 v1.16.2

第1步: 配置yum源
```shell
# 所有机器都要配置国内Kubernetes源地址 
# 执行命令:
cat <<EOF > /etc/yum.repos.d/kubernetes.repo 
[kubernetes] 
    name=Kubernetes 
    baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ 
    enabled=1 
    gpgcheck=1 
    repo_gpgcheck=1 
    gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

# 或者
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
EOF
 
yum -y install epel-release
 
yum clean all
 
yum makecache


# 如果源失效 使用配置阿里云源
# centos7 修改yum源为阿里源
# 首先是到yum源设置文件夹里
# 1. 查看yum源信息:
  yum repolist
# 2. 定位到base reop源位置
  cd /etc/yum.repos.d
# 3. 接着备份旧的配置文件
  mv CentOS-Base.repo CentOS-Base.repo.bak
# 4. 下载阿里源的文件
  wget -O CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
# ---------------------------红帽版本一样的安装------------------------------
# 安装epel repo源：
# epel(RHEL 7) 红帽7
   wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo
# epel(RHEL 6) 红帽6
   wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-6.repo
# epel(RHEL 5) 红帽5
   wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-5.repo
# ------------------------------------------------------------------------------    
# 5.清理缓存
   yum clean all
# 6.重新生成缓存
   yum makecache
# 7. 再次查看yum源信息
   yum repolist
```
第2步:所有节点安装docker,选好docker版本  如果已经安装可以跳过
```shell
yum -y install yum-utils
yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
yum list docker-ce --showduplicates|sort -r
yum install -y "docker-ce-18*"
systemctl enable docker && systemctl restart docker 
```

第3步:  参数设置和安装对应的依赖包;每台节点都要安装
```shell
# 临时关闭防火墙 
setenforce 0 
systemctl disable firewalld 
systemctl stop firewalld 
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config 
iptables -F 
iptables -F -t nat 
iptables -F -t raw 
iptables -F -t mangle 
modprobe br_netfilter 
echo "* soft nofile 655360" >> /etc/security/limits.conf 
echo "* hard nofile 655360" >> /etc/security/limits.conf 
echo "* soft nproc 655360" >> /etc/security/limits.conf 
echo "* hard nproc 655360" >> /etc/security/limits.conf 
echo "* soft memlock unlimited" >> /etc/security/limits.conf 
echo "* hard memlock unlimited" >> /etc/security/limits.conf 
echo "* hard memlock unlimited" >> /etc/security/limits.conf 
echo "DefaultLimitNOFILE=1024000" >> /etc/systemd/system.conf 
echo "DefaultLimitNPROC=1024000" >> /etc/systemd/system.conf 
echo "DefaultLimitNPROC=1024000" >> /etc/systemd/system.conf 
echo "net.bridge.bridge-nf-call-ip6tables = 1" >> /etc/sysctl.conf 
echo "net.bridge.bridge-nf-call-iptables = 1" >> /etc/sysctl.conf 
echo "net.ipv4.ip_forward = 1" >> /etc/sysctl.conf 
echo "vm.swappiness = 0" >> /etc/sysctl.conf 
sysctl -w vm.swappiness=0 
swapoff -a 
sed -i 's/.*swap.*/#&/' /etc/fstab 
sysctl -p 
yum install -y conntrack ipvsadm ipset jq sysstat curl iptables libseccomp bash-completion 
yum install -y yum-utils device-mapper-persistent-data lvm2 net-tools conntrack-tools vim libtool-ltdl 
yum install -y yum-utils device-mapper-persistent-data lvm2 net-tools conntrack-tools vim libtool-ltdl 
yum install systemd-* -y 
yum install chrony -y 
systemctl enable chronyd.service && systemctl start chronyd.service 
cp -rf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
```

第4步:每台服务器都要执行,用于docker加速和docker文件引擎配置 如果已操作 可跳过
```shell
[root@k8s-master01 k8s]# vi /etc/docker/daemon.json 
{
  "registry-mirrors": ["http://f1361db2.m.daocloud.io","https://nidf33r8.mirror.aliyuncs.com","https://docker.mirrors.ustc.edu.cn","https://hub-mirror.c.163.com","https://registry.docker-cn.com"],
  "exec-opts": ["native.cgroupdriver=systemd"]
}
```
第5步:在master节点安装如下软件(master节点)  版本要对上，这里是指定安装1.16.2，所以写死了；
```shell
yum install -y kubectl-1.16.2-0.x86_64 kubeadm-1.16.2-0.x86_64 kubelet-1.16.2-0.x86_64 kubernetes-cni-0.7.5-0.x86_64
systemctl enable kubelet && systemctl start kubelet
```

第6步: 在node节点执行
```shell
yum install -y  kubelet-1.16.2-0.x86_64  kubeadm-1.16.2-0.x86_64
systemctl enable kubelet && systemctl start kubelet
```

第7步: 在master节点上执行,查看当前kubeadm版本安装的kubernetes版本和获取镜像地址

```shell
kubeadm config print init-defaults > /root/kubeadm.conf
kubeadm config images list > /root/mages-list.txt   
# 然后查看/root/mages-list.txt 文件里的内容,可以看到默认都是官方谷歌的镜像,由于国内访问谷歌网络限制,
# 很多镜像拉不下来,因此需要将对应的国内的阿里云镜像下载下来然后打个谷歌的tag,操作的步骤如5,
# 注意比对images里的版本需要和/root/mages-list.txt 里版本对应,将/root/mages-list.txt 里的各个组件对应的版本号修为为第5步images里的版本号;
```
第8步: 在master节点上,从/root/mages-list.txt 文件里的官方镜像重新从国内pull下来并打tag  
```shell
# 新建一个文件在/root/目录下,文件名为:  k8s.sh 
# 文件内容如下: 这里以v1.16.2为例


#!/bin/bash 
images=( kube-apiserver:v1.16.2 kube-controller-manager:v1.16.2 kube-scheduler:v1.16.2 kube-proxy:v1.16.2 pause:3.1 etcd:3.3.15-0 coredns:1.6.2 kubernetes-dashboard-amd64:v1.10.0 heapster-amd64:v1.5.4 heapster-grafana-amd64:v5.0.4 heapster-influxdb-amd64:v1.5.2 pause-amd64:3.1 )
for imageName in ${images[@]} ; 
do 
     docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName 
     docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName
done 
```

第9步:给镜像打tag,执行上面的脚本 所有节点都要执行
```shell
sh -x /root/k8s.sh
```


   第10步:部署Master,在master节点执行: 注意xxx.xxx.xxx.xxx.xxx换成master节点的IP
   ```shell
   #####kubeadm init  --config /root/kubeadmin.conf####
   # pod-network-cidr=10.244.0.0/16 此处要与 flannel-k8s-v1.16.2.yaml中的 Network 保持一致  如下
   #  net-conf.json: | 
   #     {
   #       "Network": "10.244.0.0/16",
   #       "Backend": {
   #         "Type": "vxlan"
   #       }
   #     }
   
   # 初始化k8s之前先删除 /var/lib/etcd/下的文件 不然启动会报错
   rm -rf ./*  /var/lib/etcd/
   
   kubeadm init --apiserver-advertise-address=192.168.238.129 --kubernetes-version v1.16.2  --pod-network-cidr=10.244.0.0/16  --service-cidr=10.96.0.0/16
   
   # 启动成功会有提示
   
   
   Your Kubernetes control-plane has initialized successfully!
   
   To start using your cluster, you need to run the following as a regular user:
   
     mkdir -p $HOME/.kube
     sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
     sudo chown $(id -u):$(id -g) $HOME/.kube/config
   
   You should now deploy a pod network to the cluster.
   Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
     https://kubernetes.io/docs/concepts/cluster-administration/addons/
   
   Then you can join any number of worker nodes by running the following on each as root:
   
   # 此处为给node节点注册用
   kubeadm join 192.168.81.128:6443 --token 6htrdv.xpsc2r7ez9wmbeto \
       --discovery-token-ca-cert-hash sha256:ec39d74ca1dab2ce811e1d569cb0a71fc833342b8a97709b59b7ddf38dde3749
       
   # 按照步骤 创建
     mkdir -p $HOME/.kube
     sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
     sudo chown $(id -u):$(id -g) $HOME/.kube/config
   ```
   第11步:在master节点执行:master节点要安装flannel： 
   ```shell
   # 这个是从官网下载的最新版本的flannel，
   kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
   
   # 可以用下面这个
   kubectl apply -f   flannel-k8s-v1.16.2.yaml
   ```

   >  flannel-k8s-v1.16.2.yaml 文件内容:

   ```yaml
   ---
   apiVersion: policy/v1beta1
   kind: PodSecurityPolicy
   metadata:
     name: psp.flannel.unprivileged
     annotations:
       seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default
       seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default
       apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default
       apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default
   spec:
     privileged: false
     volumes:
     - configMap
     - secret
     - emptyDir
     - hostPath
     allowedHostPaths:
     - pathPrefix: "/etc/cni/net.d"
     - pathPrefix: "/etc/kube-flannel"
     - pathPrefix: "/run/flannel"
     readOnlyRootFilesystem: false
     # Users and groups
     runAsUser:
       rule: RunAsAny
     supplementalGroups:
       rule: RunAsAny
     fsGroup:
       rule: RunAsAny
     # Privilege Escalation
     allowPrivilegeEscalation: false
     defaultAllowPrivilegeEscalation: false
     # Capabilities
     allowedCapabilities: ['NET_ADMIN', 'NET_RAW']
     defaultAddCapabilities: []
     requiredDropCapabilities: []
     # Host namespaces
     hostPID: false
     hostIPC: false
     hostNetwork: true
     hostPorts:
     - min: 0
       max: 65535
     # SELinux
     seLinux:
       # SELinux is unused in CaaSP
       rule: 'RunAsAny'
   ---
   kind: ClusterRole
   apiVersion: rbac.authorization.k8s.io/v1
   metadata:
     name: flannel
   rules:
   - apiGroups: ['extensions']
     resources: ['podsecuritypolicies']
     verbs: ['use']
     resourceNames: ['psp.flannel.unprivileged']
   - apiGroups:
     - ""
     resources:
     - pods
     verbs:
     - get
   - apiGroups:
     - ""
     resources:
     - nodes
     verbs:
     - list
     - watch
   - apiGroups:
     - ""
     resources:
     - nodes/status
     verbs:
     - patch
   ---
   kind: ClusterRoleBinding
   apiVersion: rbac.authorization.k8s.io/v1
   metadata:
     name: flannel
   roleRef:
     apiGroup: rbac.authorization.k8s.io
     kind: ClusterRole
     name: flannel
   subjects:
   - kind: ServiceAccount
     name: flannel
     namespace: kube-system
   ---
   apiVersion: v1
   kind: ServiceAccount
   metadata:
     name: flannel
     namespace: kube-system
   ---
   kind: ConfigMap
   apiVersion: v1
   metadata:
     name: kube-flannel-cfg
     namespace: kube-system
     labels:
       tier: node
       app: flannel
   data:
     cni-conf.json: |
       {
         "name": "cbr0",
         "cniVersion": "0.3.1",
         "plugins": [
           {
             "type": "flannel",
             "delegate": {
               "hairpinMode": true,
               "isDefaultGateway": true
             }
           },
           {
             "type": "portmap",
             "capabilities": {
               "portMappings": true
             }
           }
         ]
       }
     net-conf.json: |
       {
         "Network": "10.244.0.0/16",
         "Backend": {
           "Type": "vxlan"
         }
       }
   ---
   apiVersion: apps/v1
   kind: DaemonSet
   metadata:
     name: kube-flannel-ds
     namespace: kube-system
     labels:
       tier: node
       app: flannel
   spec:
     selector:
       matchLabels:
         app: flannel
     template:
       metadata:
         labels:
           tier: node
           app: flannel
       spec:
         affinity:
           nodeAffinity:
             requiredDuringSchedulingIgnoredDuringExecution:
               nodeSelectorTerms:
               - matchExpressions:
                 - key: kubernetes.io/os
                   operator: In
                   values:
                   - linux
         hostNetwork: true
         priorityClassName: system-node-critical
         tolerations:
         - operator: Exists
           effect: NoSchedule
         serviceAccountName: flannel
         initContainers:
         - name: install-cni
           image: quay.io/coreos/flannel:v0.13.0
           command:
           - cp
           args:
           - -f
           - /etc/kube-flannel/cni-conf.json
           - /etc/cni/net.d/10-flannel.conflist
           volumeMounts:
           - name: cni
             mountPath: /etc/cni/net.d
           - name: flannel-cfg
             mountPath: /etc/kube-flannel/
         containers:
         - name: kube-flannel
           image: quay.io/coreos/flannel:v0.13.0
           command:
           - /opt/bin/flanneld
           args:
           - --ip-masq
           - --kube-subnet-mgr
           resources:
             requests:
               cpu: "100m"
               memory: "50Mi"
             limits:
               cpu: "100m"
               memory: "50Mi"
           securityContext:
             privileged: false
             capabilities:
               add: ["NET_ADMIN", "NET_RAW"]
           env:
           - name: POD_NAME
             valueFrom:
               fieldRef:
                 fieldPath: metadata.name
           - name: POD_NAMESPACE
             valueFrom:
               fieldRef:
                 fieldPath: metadata.namespace
           volumeMounts:
           - name: run
             mountPath: /run/flannel
           - name: flannel-cfg
             mountPath: /etc/kube-flannel/
         volumes:
         - name: run
           hostPath:
             path: /run/flannel
         - name: cni
           hostPath:
             path: /etc/cni/net.d
         - name: flannel-cfg
           configMap:
             name: kube-flannel-cfg
   
   ```

   第12步:在node节点上执行kubeadmin join xxxxxxxxxx

   ```shell
   # 在第11步执行后,最后的日志里会打印如何将node节点加入k8s集群中;kubeadmin join xxxxxxxx
   # 按照提示这一步完成后提示 把node节点加进去
   
   # 此处为给node节点注册用
   kubeadm join 192.168.81.128:6443 --token 6htrdv.xpsc2r7ez9wmbeto \
       --discovery-token-ca-cert-hash sha256:ec39d74ca1dab2ce811e1d569cb0a71fc833342b8a97709b59b7ddf38dde3749
   ```

   ```shell
   # 出于安全考虑，默认配置下Kubernetes不会将Pod调度到Master节点。如果希望将k8s-master也当作Node使用，可以执行如下命令：
   
   kubectl taint node k8s-master-01 node-role.kubernetes.io/master-1
   # 其中k8s-master是主机节点hostname如果要恢复Master Only状态，执行如下命令：
   
   kubectl taint node k8s-master-01 node-role.kubernetes.io/master=""
   ```

   #### 安装失败操作

   ```shell
    kubeadm reset
    
    rm -rf /var/lib/etcd /var/lib/kubelet /etc/cni/net.d /var/lib/dockershim /var/run/kubernetes /var/lib/cni
    
    rm -rf $HOME/.kube
    
    service kubelet restart
   # master 重新初始化
    
   # node 重新join
   ```

   

   

   ## rancher 安装并导入 k8s 集群

   ### rancher 安装

   ```shell
   # 指定版本安装 不指定:v2.4.8 则安装最新版本
   docker pull rancher/rancher:v2.4.8
   
   # 启动
   docker run --privileged -d --restart=unless-stopped -p 80:80 -p 443:443 \
   --name rancher -e JAVA_OPTS="-Xmx1024m" rancher/rancher:v2.4.8
   
   # 浏览器访问 所安装地址主机
   # 设置密码 rancher 用户名为 admin
   ```

   ### 导入k8s集群

   #### 添加集群 

   1. 点击右上角添加集群

   2. 点击导入

   3. 输入集群名称 local-k8s

   4. 点击创建

   5. 在现有的受支持的 Kubernetes 集群上运行下面的 kubectl 命令，将其导入 Rancher: 复制命令

      ```shell
      kubectl apply -f https://192.168.238.129/v3/import/kpp2p7dt26prj2r6876f54gr64z5457cgd55zjj9brspckhjc4jv4f.yaml
      # 如果报错 可以将https://192.168.238.129/v3/import/kpp2p7dt26prj2r6876f54gr64z5457cgd55zjj9brspckhjc4jv4f.yaml下载下来 
      # 上传到服务器并且命名为 local-k8s.yaml 再重新执行命令即可：
      kubectl apply -f local-k8s.yaml
      ```

   6. 如果由于您的 Rancher 安装使用不受信任/自签名的 SSL 证书而出现“由未知权限签名的证书”错误，请运行下面的命令以绕过证书检查:

      ```shell
      curl --insecure -sfL https://192.168.238.129/v3/import/kpp2p7dt26prj2r6876f54gr64z5457cgd55zjj9brspckhjc4jv4f.yaml | kubectl apply -f -
      
      # 如果报错 可以将https://192.168.238.129/v3/import/kpp2p7dt26prj2r6876f54gr64z5457cgd55zjj9brspckhjc4jv4f.yaml下载下来 
      # 上传到服务器并且命名为 local-k8s-sfl.yaml 再重新执行命令即可：
      curl --insecure -sfL local-k8s-sfl.yaml | kubectl apply -f -
      ```

   ### 部署服务

   1. 选择集群 local-k8s 点击 default
   2. 点击部署服务
   3. 输入名称 例 nginx
   4. Docker 镜像 输入 nginx   （docker pull nginx）
   5. 端口映射
      - 端口名称 80tcp
      - 容器端口 80
      - 协议 tcp
      - 网络模式 NodePort
      - 主机监听端口 30080
   6. 浏览器访问 nginx  ip:30080 则成功部署

### rancher 部署 jenkins


```shell
# 在宿主机创建挂载文件夹
mkdir -p /water/runfile/docker_volume/jenkins_home

# 在rancher的部署页中做如下操作
	1. 输入名称 jenkins
	2. 输入镜像名称 jenkins/jenkins
	3. 添加端口映射 30000(主机):8080(容器)和 30001(主机):50000(容器)
	4. 添加一个路径映射卷，卷名为jenkins-home主机路径的话就是填入我们先前创建的/water/runfile/docker_volume/jenkins_home目录路径，选择为现有目录并映射到容器路径/var/jenkins_home目录路径
		4.0.1后面maven的配置文件
	4.1 因为要在jenkins下执行docker命令，所以再挂载两个宿主机的路径/var/run/docker.sock:/var/run/docker.sock /usr/bin/docker:/usr/bin/docker
	4.2 因为jenkins里要执行docker命令，所以将用户设置为uid为0的(表示使用root权限)用户启动。点击高级选项，命令，用户uid填写0
	4.3 环境变量设置 key = JAVA_OPTS ; value = -Dhudson.model.DownloadService.noSignatureCheck=true -Duser.timezone=Asia/Shanghai
	5. 点击启动按钮

# 这几个步骤其实反应到我们docker容器中就如同下面的命令：
docker run --privileged -d --restart unless-stopped --name jenkins \
    -u 0 -p 30000:8080 -p 30001:50000 \
    -v /water/runfile/docker_volume/jenkins_home:/var/jenkins_home \
    jenkins:2.60.3-alpine


# 刷新等待，直到等到服务状态为Active后说明服务部署成功

# 你可以点击如下图中标红的连接http://192.168.232.140:30000/ 就是我们先前对8080映射到了主机30000端口

# 如果一直出启动中 按照网页中更改下载源
https://www.cnblogs.com/brady-wang/p/11732850.html
https://www.cnblogs.com/gaozejie/p/15781379.html

# 初始化Jenkins
# 到我们的主机映射目录去查看密码
cat /water/runfile/docker_volume/jenkins_home/secrets/initialAdminPassword

# 当然也可以在rancher中操作选项里操作执行命令行，在这个里面操作命令就是基于容器内部文件路径了


# 选择"按照系统建议的插件"，一直等到插件安装完成，这可能需要几分钟时间
# 非常遗憾默认的插件里没有我们用到的maven，后面需要自己配置插件，
# 从这里我们也要思考一下以后在做项目中，能够考虑用Gradle来替代maven，这也许是一个流行趋势

# 创建用户设置密码
root
root

# 安装maven
# 在全局工具配置中安装maven， 选择一个合适的版本，勾选自动安装，之后直接保存，
# 需注意的是，现在jenkins并不会立即给你安装maven软件

# maven名字为：jenkins-in-maven

# 接下来我们在插件管理中查找maven插件，
# 可以在浏览器中使用ctrl+f快捷键来快速定位插件，
# 选择好maven integration插件，然后点击直接安装

# 创建一个maven项目进行一下构建，这里构建肯定会失败，因为没有指定仓库等信息
# 配置git地址， 命令clean package -Dmaven.test.skip=true

# 构建后会自动下载maven.zip

# 在正式构建项目代码前，需要配置maven的仓库等信息
# 在宿主机执行如下命令进入jenkins-in-maven目录(先前安装的默认目录) 并通过ls查看目录下清单列表
# cd /water/runfile/docker_volume/jenkins_home/tools/hudson.tasks.Maven_MavenInstallation/jenkins-in-maven

# 指定Repository目录和mirror等配置
vi conf/settings.xml
# 在相应位置加入如下配置，并保存退出vi模式

<localRepository>
/var/jenkins_home/tools/hudson.tasks.Maven_MavenInstallation/jenkins-in-maven/repository
</localRepository>

# 与

<mirror>
  <id>alimaven</id>
  <name>aliyun maven</name>
  <url>http://maven.aliyun.com/nexus/content/groups/public/</url>
  <mirrorOf>central</mirrorOf>
</mirror>

# 在<mirrors>标签下

# 配置好后继续点击构建

```

### rancher 部署 docker 镜像仓库

```shell
# 搭建本地docker 镜像仓库
docker pull registry

# 在每一台节点加入一下信息
vi /etc/docker/daemon.json 
{
    "registry-mirrors": ["http://86d2a50b.m.daocloud.io"],
    "insecure-registries": ["192.168.238.129:5000"]
}
# 重启docker
systemctl  restart docker

# 最后在 rancher中配置 registry

1、服务名 registry
2、镜像名registry:latest
3、端口 5000 选择 hostPord 主机端口 5000
4. 映射卷路径 /water/runfile/docker_volume/registry:/var/lib/registry
```

### dockerfile 文件编辑

```dockerfile
# 文件路径 java工程下 /src/main/docker/Dockerfile
FROM openjdk:8
EXPOSE 9999
VOLUME /tmp
ADD *.jar app.jar
RUN sh -c 'touch /app.jar'
ENTRYPOINT [ "sh", "-c", "java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar /app.jar" ]
```

### jenkins 构建Java应用

```shell
# 1. 创建maven项目 (可用流水线作业等 根据个人需求)

# 2. 输入名称 devpos

# 3. 源码管理 填写git
	# 3.1 Repository URL
		https://gitee.com/hq-study/devpos.git

	# 3.2 添加凭据 
		username = huang-qing -- password = ******

	# 3.3 指定分支 (根据个人仓库分支)
		*/master 

# 4. Pre Steps add pre-build step 选择调用顶层Maven目标 
  # 4.1 Maven 版本 选择全局配置的maven版本：
 	jenkins-in-maven
  # 4.2 目标
   	clean package -Dmaven.test.skip=true
  # 4.2 继续 add pre-build step 选择 执行shell
  	# 此处如果你的仓库是远程仓库 则加上本地仓库无需添加 ↓
  	# docker login --username=******** --password ********
  	
    cp ${WORKSPACE}/src/main/docker/Dockerfile ${WORKSPACE}/target
    cd ${WORKSPACE}/target
    docker build . -t devpos:v1.0
    docker tag devpos:v1.0 192.168.238.129:5000/devpos:v1.0
    docker push 192.168.238.129:5000/devpos:v1.0
```

### rancher 部署java项目

```shell
1、服务名 devpos
2、镜像名 192.168.238.129:5000/devpos:v1.0
3、端口 8080 选择 nodeport 主机端口 30003
4、环境变量 key=JAVA_OPTS value=-Duser.timezone=Asia/Shanghai
5、启动后访问 部署主机IP:30003/devpos 成功返回信息则完成
```

### jenkins 构建node.js应用

- 项目结构

  ```java
  |-- docs
  |   |-- README.md
  |   |-- all
  |       |-- Jvm-指南.md
      |-- .vuepress
          |-- public
      		|-- logo.jpg
          	|-- my.jpg
              |-- img
              	|-- favicon.ico
          |-- config.js
  |-- deploy.sh
  |-- nginx.conf
  |-- package-lock.json
  |-- package.json
  ```

  

- **dockerfile 文件编辑**

  ```dockerfile
  FROM nginx
  RUN mkdir /app
  COPY /docs/.vuepress/dist /app
  COPY nginx.conf /etc/nginx/nginx.conf
  CMD ["nginx", "-g", "daemon off;"]
  ```

- **nginx.conf**

  ```nginx
  user  nginx;
  worker_processes  1;
  error_log  /var/log/nginx/error.log warn;
  pid        /var/run/nginx.pid;
  events {
    worker_connections  1024;
  }
  http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;
    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';
    access_log  /var/log/nginx/access.log  main;
    sendfile        on;
    keepalive_timeout  65;
    server {
      listen       8090;
      server_name  localhost;
      location / {
        root   /app;
        index  index.html;
        try_files $uri $uri/ /index.html;
      }
      error_page   500 502 503 504  /50x.html;
      location = /50x.html {
        root   /usr/share/nginx/html;
      }
    }
  }
  ```

- **操作步骤**

```shell
# 安装 nodejs 插件
# 全局配置中 安装nodeJs 版本 名称nodeJs16.14.2 我这里使用的16.14.2
# 创建一个自由项目进行一下构建，在 构建环境 中 勾选 Provide Node & npm bin/ folder to PATH 选择nodeJs16.14.2  其它默认 保存
# 点击构建后会自动下载 nodeJs16.14.2


# 1. 构建一个自由风格的软件项目 (可用流水线作业等 根据个人需求)

# 2. 输入名称 devpos

# 3. 源码管理 填写git
	# 3.1 Repository URL
		https://gitee.com/hq-study/blogs.git

	# 3.2 添加凭据 
		username = huang-qing -- password = ******

	# 3.3 指定分支 (根据个人仓库分支)
		*/master 
# 4. 在 构建环境 中 勾选 Provide Node & npm bin/ folder to PATH 选择nodeJs16.14.2  其它默认
# 5. add pre-build step 选择 执行shell
	 npm config set sass_binary_site=https://npm.taobao.org/mirrors/node-sass
     npm install --registry=https://registry.npm.taobao.org
     npm run docs:build
# 6. add pre-build step 选择 执行shell
  	# 此处如果你的仓库是远程仓库 则加上本地仓库无需添加 ↓
  	# docker login --username=******** --password ********
    docker build . -t blogs:v1.0
    docker tag blogs:v1.0 192.168.238.129:5000/blogs:v1.0
    docker push 192.168.238.129:5000/blogs:v1.0
```

### rancher 部署node.js项目

```shell
1、服务名 blogs
2、镜像名 192.168.238.129:5000/devpos:v1.0
3、端口 8090 选择 nodeport 主机端口 30004
4、启动后访问 部署主机IP:30004 显示页面 则表示成功
```

